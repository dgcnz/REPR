# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: im1k
  - override /model: pretrain/partmae_v2/vit_b_16_224
  - override /data/transform@data.train_transform: DropPos/in1k_pretrain_train
  - override /data/transform@data.test_transform: DropPos/in1k_pretrain_test
  - override /callbacks: pretrain/default
  - override /trainer: gpu
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["pretrain", "imagenet1k", "partmae_v2", "vit_b_16"]

seed: 12345
float32_matmul_precision: "high"

trainer:
  min_epochs: ???
  max_epochs: ???
  check_val_every_n_epoch: 1
  deterministic: true
  accumulate_grad_batches: ???

blr: 1.5e-4

model:
  net:
    img_size: 224
    mask_ratio: 0.75
    pos_mask_ratio: 0.75
    sampler: stratified_jittered
    criterion: l1

  scheduler_interval: epoch 
  scheduler:
    warmup_t: ??? 
    t_initial: 800

  optimizer:
    lr: ${eval:${blr} * ${data.batch_size} * ${trainer.accumulate_grad_batches} / 256}
    weight_decay: 0.05

data:
  batch_size: ???
  num_workers: ???


logger:
  wandb:
    project: "PARTv2-pretrain"
    group: "imagenet1k/partmae_v2/vit_b_16"