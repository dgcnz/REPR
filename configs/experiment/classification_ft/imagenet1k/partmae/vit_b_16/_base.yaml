# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: im1k
  - override /model: classification_ft/vit_b_16_224
  - override /callbacks: classification_ft/default
  - override /trainer: gpu
  - override /logger: wandb
  - override /data/transform@data.train_transform: DropPos/vitb_im1k_finetune
  - override /data/mixup: DropPos/vitb_im1k_finetune

tags: ["classification_ft", "imagenet1k", "partmae", "vit_b_16"]

seed: 12345
float32_matmul_precision: "high"

trainer:
  min_epochs: ???
  max_epochs: ???
  check_val_every_n_epoch: 1
  deterministic: true

model:
  num_classes: 1000
  net:
    pos_embed: 'learn'
    class_token: True
    drop_path_rate: 0.1 # from droppos finetune 1k
    pretrained_cfg_overlay:
      file: ???
  scheduler_interval: epoch
  scheduler:
    t_initial: 100 # finetune for 100 epochs
  optimizer:
    weight_decay: 0.02

data:
  batch_size: ???
  num_workers: ???
  test_fraction: 0.2
  mixup:
    num_classes: 1000

logger:
  wandb:
    project: "PARTv2-classification-ft"
    group: "imagenet1k/partmae/vit_b_16"

