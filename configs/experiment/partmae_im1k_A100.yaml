# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: im1k
  - override /model: part_mae
  - override /callbacks: training
  - override /trainer: ddp
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["part", "mae", "a100", "snellius"]

seed: 12345
float32_matmul_precision: "high"

trainer:
  min_epochs: 15
  max_epochs: 200
  check_val_every_n_epoch: 1
  deterministic: true
  devices: 4
  num_nodes: 1
  sync_batchnorm: true

  strategy:
    _target_: src.strategies.ddp_compile.DDPCompileStrategy


model:
  net:
    backbone:
      img_size: 224
      patch_size: 16
  sample_mode: offgrid

  optimizer:
    lr: 1e-4
  scheduler:
    warmup_t: 5 
  scheduler_interval: epoch


data:
  batch_size: 1024
  num_workers: 16
  cache_dir: /scratch-shared/dcanez/HF_HOME


logger:
  wandb:
    group: part_mae_im1k_A100