{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa0ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434117c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    V = 2\n",
    "    RUN_ID = \"nw6nhpa2\"\n",
    "    EPOCH = \"0199\"\n",
    "    ckpt = torch.load(f\"../../artifacts/model-{RUN_ID}:v0/epoch_{EPOCH}.ckpt\")[\"model\"]\n",
    "    ckpt[\"pose_head.linear.weight\"] = ckpt.pop(\"decoder_pred.weight\")\n",
    "    ckpt[\"_patch_loss.sigma\"] = torch.tensor([0.1, 0.1, 0.1, 0.1])\n",
    "    # ckpt[\"segment_embed\"] = ckpt[\"segment_embed\"][:V]\n",
    "    ckpt[\"segment_embed\"] = torch.zeros_like(ckpt[\"segment_embed\"])[:V]\n",
    "\n",
    "    cfg = OmegaConf.load(f\"../../artifacts/model-{RUN_ID}:v0/config.yaml\")\n",
    "    cfg[\"model\"][\"segment_embed_mode\"] = (\n",
    "        \"permute\" if cfg[\"model\"].pop(\"permute_segment_embed\") else \"fixed\"\n",
    "    )\n",
    "    model = hydra.utils.instantiate(\n",
    "        cfg[\"model\"],\n",
    "        _target_=\"src.models.components.partmae_v6.PARTMaskedAutoEncoderViT\",\n",
    "        verbose=True,\n",
    "        num_views=V,\n",
    "        # lambda_dino=0.1\n",
    "    )\n",
    "    model.load_state_dict(ckpt, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d78832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb546df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: torch.nn.Module = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4630522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dino_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6560c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.utils.partmae_v6_simdino import load_partmaev6_from_simdino\n",
    "\n",
    "ckpt_path = \"../../artifacts/vitb16_SimDINOv1_gpu8_bs64_ep100.pth\"\n",
    "# _ = load_partmaev6_from_simdino(model, ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6329e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = {\n",
    "    \"global_step\": 5,\n",
    "}\n",
    "torch.save(new_state, \"new_state.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a42a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The requested state contains a key 'model' that does not exist in the loaded checkpoint. To disable strict loading, set `strict=False`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfabric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_state.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/development/thesis/PART/.venv/lib/python3.10/site-packages/lightning/fabric/fabric.py:767\u001b[0m, in \u001b[0;36mFabric.load\u001b[0;34m(self, path, state, strict)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03mHow and which processes load gets determined by the `strategy`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    764\u001b[0m \n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m unwrapped_state \u001b[38;5;241m=\u001b[39m _unwrap_objects(state)\n\u001b[0;32m--> 767\u001b[0m remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munwrapped_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;66;03m# We need to unwrap objects (see above) but this creates a new dictionary. In-place updates\u001b[39;00m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;66;03m# (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.\u001b[39;00m\n",
      "File \u001b[0;32m~/development/thesis/PART/.venv/lib/python3.10/site-packages/lightning/fabric/strategies/strategy.py:342\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[0;34m(self, path, state, strict)\u001b[0m\n\u001b[1;32m    339\u001b[0m     state\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 342\u001b[0m \u001b[43m_validate_keys_for_strict_loading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "File \u001b[0;32m~/development/thesis/PART/.venv/lib/python3.10/site-packages/lightning/fabric/strategies/strategy.py:451\u001b[0m, in \u001b[0;36m_validate_keys_for_strict_loading\u001b[0;34m(requested_keys, checkpoint_keys, strict)\u001b[0m\n\u001b[1;32m    449\u001b[0m invalid_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m requested_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m checkpoint_keys]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m invalid_keys:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe requested state contains a key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_keys[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that does not exist in the loaded checkpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m To disable strict loading, set `strict=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The requested state contains a key 'model' that does not exist in the loaded checkpoint. To disable strict loading, set `strict=False`.\""
     ]
    }
   ],
   "source": [
    "from lightning.fabric import Fabric\n",
    "fabric = Fabric()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "state = {\n",
    "    \"model\": torch.ones(3),\n",
    "    \"global_step\": 2,\n",
    "}\n",
    "print(state[\"global_step\"])\n",
    "state_dict = fabric.load(\"new_state.pth\", state, strict=True)\n",
    "print(state[\"global_step\"])\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7ef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "dict_keys(['teacher'])\n",
      "tensor(0.1227, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "RUN_ID = \"nw6nhpa2\"\n",
    "cfg = OmegaConf.load(f\"../../artifacts/model-{RUN_ID}:v0/config.yaml\")\n",
    "cfg[\"model\"][\"segment_embed_mode\"] = (\n",
    "    \"permute\" if cfg[\"model\"].pop(\"permute_segment_embed\") else \"fixed\"\n",
    ")\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg[\"model\"],\n",
    "    _target_=\"src.models.components.utils.partmae_v6_simdino.PARTMaskedAutoEncoderViTFromSimDINO\",\n",
    "    verbose=True,\n",
    "    num_views=2,\n",
    "    lambda_dino=0.1\n",
    ")\n",
    "state = {\"model\": model}\n",
    "ckpt_path = \"../../artifacts/vitb16_SimDINOv1_gpu8_bs64_ep100.pth\"\n",
    "state_dict = torch.load(ckpt_path)\n",
    "fabric = Fabric()\n",
    "print(model.dino_head.mlp[4].bias.sum())\n",
    "print(state_dict.keys())\n",
    "fabric.load_raw(ckpt_path, model)\n",
    "print(model.dino_head.mlp[4].bias.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b558c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"global_step\"]\n",
    "state[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22247b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(\"../../artifacts/vitb16_SimDINOv1_gpu8_bs64_ep100.pth\")\n",
    "# state_dict = ckpt[\"teacher\"]\n",
    "# \n",
    "# # 1. Remove the prefix \"_orig_mod.\" from the keys\n",
    "# unwanted_prefix = \"_orig_mod.\"\n",
    "# for k, v in list(state_dict.items()):\n",
    "#     if k.startswith(unwanted_prefix):\n",
    "#         state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "# \n",
    "# # state_dict.keys()\n",
    "# # 2. replace prefix head with dino_head\n",
    "# for k, v in list(state_dict.items()):\n",
    "#     if k.startswith(\"head.\"):\n",
    "#         state_dict[k.replace(\"head.\", \"dino_head.\")] = state_dict.pop(k)\n",
    "# \n",
    "# # 3. replace remove prefix  backbone\n",
    "# for k, v in list(state_dict.items()):\n",
    "#     if k.startswith(\"backbone.\"):\n",
    "#         state_dict[k.replace(\"backbone.\", \"\")] = state_dict.pop(k)\n",
    "# \n",
    "# # 4. reshape patch_embed.proj.weight\n",
    "# state_dict[\"patch_embed.proj.weight\"] = state_dict[\"patch_embed.proj.weight\"].reshape(\n",
    "#     state_dict[\"patch_embed.proj.weight\"].shape[0], -1\n",
    "# )\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
