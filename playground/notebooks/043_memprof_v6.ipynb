{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import re\n",
    "import contextlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "TIME_FORMAT_STR = \"%b_%d_%H_%M_%S\"\n",
    "\n",
    "class MemoryProfiler:\n",
    "    def __init__(self, filter_keyword):\n",
    "        \"\"\"\n",
    "        Initialize the MemoryProfiler.\n",
    "\n",
    "        Args:\n",
    "            filter_keyword (str): Keyword to filter profiler events.\n",
    "        \"\"\"\n",
    "        self.filter_keyword = filter_keyword\n",
    "        self.prof = None\n",
    "        self.df_events = None\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_stack(stack):\n",
    "        \"\"\"\n",
    "        Parse a stack list to extract nn.Module names.\n",
    "        Returns a string with module names (from innermost to outer) separated by '/'.\n",
    "        \"\"\"\n",
    "        pat = re.compile(r\"nn.Module: ([\\w\\d_]+)\")\n",
    "        modules = [m.group(1) for m in (pat.match(s) for s in stack) if m is not None]\n",
    "        return \"/\".join(modules[::-1])\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def profile(self, profiler_kwargs=None):\n",
    "        \"\"\"\n",
    "        A context manager for profiling a block of code.\n",
    "        \n",
    "        Usage:\n",
    "            with profiler.profile():\n",
    "                # Code to profile, e.g., model(*input)\n",
    "        \"\"\"\n",
    "        # Combine default kwargs with any overrides\n",
    "        kwargs = dict()\n",
    "        if profiler_kwargs:\n",
    "            kwargs.update(profiler_kwargs)\n",
    "            \n",
    "        self.prof = torch.profiler.profile(\n",
    "            activities=[\n",
    "                torch.profiler.ProfilerActivity.CPU,\n",
    "                torch.profiler.ProfilerActivity.CUDA,\n",
    "            ],\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True,\n",
    "            with_modules=True,\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True),\n",
    "            **kwargs\n",
    "        )\n",
    "        torch.cuda.memory._record_memory_history(max_entries=1000000)   # start\n",
    "        self.prof.__enter__()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.prof.__exit__(None, None, None)\n",
    "\n",
    "    def extract_events_df(self):\n",
    "        \"\"\"\n",
    "        Extract profiler events as a pandas DataFrame.\n",
    "        Returns a DataFrame with columns for start/end times (ms), event name, memory (GB),\n",
    "        the full stack, a short version of the stack, parsed module hierarchy, and duration.\n",
    "        \"\"\"\n",
    "        evs = [ev for ev in self.prof.events() if self.filter_keyword in \"\\n\".join(ev.stack)]\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"cpu_start\": ev.time_range.start / 1e3,\n",
    "                    \"cpu_end\": ev.time_range.end / 1e3,\n",
    "                    \"name\": ev.name,\n",
    "                    \"id\": ev.id,\n",
    "                    \"memory\": (ev.device_memory_usage / 1024**3) if ev.device_memory_usage is not None else 0,\n",
    "                    \"stack\": \"\\n\".join(ev.stack),\n",
    "                    \"short_stack\": \"\\n\".join(ev.stack[5:]),\n",
    "                    \"module\": self.parse_stack(ev.stack),\n",
    "                }\n",
    "                for ev in evs\n",
    "            ]\n",
    "        )\n",
    "        df[\"delta\"] = df[\"cpu_end\"] - df[\"cpu_start\"]\n",
    "        self.df_events = df.sort_values(\"cpu_start\")\n",
    "        return self.df_events\n",
    "\n",
    "    def plot_timeline(self):\n",
    "        \"\"\"\n",
    "        Create an interactive timeline plot of events using Plotly.\n",
    "        Each event is represented as a box from its start to end time and colored by its memory usage.\n",
    "        Hover data shows the parsed module hierarchy.\n",
    "        \"\"\"\n",
    "        if self.df_events is None:\n",
    "            raise ValueError(\"No events extracted. Run extract_events_df() first.\")\n",
    "        fig = px.timeline(\n",
    "            self.df_events,\n",
    "            x_start=\"cpu_start\",\n",
    "            x_end=\"cpu_end\",\n",
    "            y=\"name\",\n",
    "            color=\"memory\",\n",
    "            title=\"Memory Usage Timeline\",\n",
    "            hover_data=[\"module\"],\n",
    "        )\n",
    "        fig.update_yaxes(visible=False)\n",
    "        fig.layout.xaxis.type = \"linear\"\n",
    "        # Adjust widths of boxes using the event durations.\n",
    "        # explicit override of both start (base) and width (x)\n",
    "        fig.update_traces(\n",
    "            base=self.df_events[\"cpu_start\"].tolist(),\n",
    "            x   =self.df_events[\"delta\"].tolist(),\n",
    "            selector=dict(type=\"bar\")   # only affect bar traces\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    def plot_cumulative_memory(self):\n",
    "        \"\"\"\n",
    "        Plot the accumulated memory allocations over time.\n",
    "        Note: This is a cumulative sum and may exceed the device's total memory.\n",
    "        \"\"\"\n",
    "        if self.df_events is None:\n",
    "            raise ValueError(\"No events extracted. Run extract_events_df() first.\")\n",
    "        df = self.df_events.copy()\n",
    "        df[\"cumulative_memory\"] = df[\"memory\"].cumsum()\n",
    "        fig = px.area(\n",
    "            df,\n",
    "            x=\"cpu_start\",\n",
    "            y=\"cumulative_memory\",\n",
    "            title=\"Accumulated GPU Memory Allocation Over Time\",\n",
    "            labels={\"cpu_start\": \"Time (ms)\", \"cumulative_memory\": \"Accumulated Memory (GB)\"},\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    def compute_net_memory_usage(self):\n",
    "        \"\"\"\n",
    "        Compute the net memory usage (i.e., memory in use) over time.\n",
    "        Each event contributes a positive delta at its start and a negative delta at its end.\n",
    "        Returns a DataFrame with time stamps and net memory usage.\n",
    "        \"\"\"\n",
    "        if self.df_events is None:\n",
    "            raise ValueError(\"No events extracted. Run extract_events_df() first.\")\n",
    "        changes = []\n",
    "        for _, row in self.df_events.iterrows():\n",
    "            changes.append((row[\"cpu_start\"], row[\"memory\"]))\n",
    "            changes.append((row[\"cpu_end\"], -row[\"memory\"]))\n",
    "        changes.sort(key=lambda x: x[0])\n",
    "        times, net_memory = [], []\n",
    "        current = 0\n",
    "        for t, delta in changes:\n",
    "            current += delta\n",
    "            times.append(t)\n",
    "            net_memory.append(current)\n",
    "        net_df = pd.DataFrame({\"time\": times, \"net_memory\": net_memory})\n",
    "        return net_df\n",
    "\n",
    "    def plot_net_memory_usage(self):\n",
    "        \"\"\"\n",
    "        Create an area plot showing the net GPU memory usage over time.\n",
    "        This plot represents the actual memory used at each moment.\n",
    "        \"\"\"\n",
    "        net_df = self.compute_net_memory_usage()\n",
    "        fig = px.area(\n",
    "            net_df,\n",
    "            x=\"time\",\n",
    "            y=\"net_memory\",\n",
    "            title=\"Net GPU Memory Usage Over Time\",\n",
    "            labels={\"time\": \"Time (ms)\", \"net_memory\": \"Memory Usage (GB)\"},\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    def plot_record_functions_timeline(self, record_fn_names=[\"forward\", \"backward\"]):\n",
    "        \"\"\"\n",
    "        Create an interactive timeline plot for recorded function events using Plotly.\n",
    "        Each record_function event is represented as a box spanning its start to end time.\n",
    "        If no events are found, the method prints all recorded event names for debugging.\n",
    "        \"\"\"\n",
    "        # Normalize the record function names to lower case.\n",
    "        target_names = [name.lower() for name in record_fn_names]\n",
    "        record_events = [ev for ev in self.prof.events() if ev.name and ev.name.lower() in target_names]\n",
    "        df_record = pd.DataFrame(\n",
    "            {\n",
    "                \"cpu_start\": [ev.time_range.start / 1e3 for ev in record_events],\n",
    "                \"cpu_end\": [ev.time_range.end / 1e3 for ev in record_events],\n",
    "                \"name\": [ev.name for ev in record_events],\n",
    "            }\n",
    "        )\n",
    "        df_record[\"delta\"] = df_record[\"cpu_end\"] - df_record[\"cpu_start\"]\n",
    "        fig = px.timeline(\n",
    "            df_record,\n",
    "            x_start=\"cpu_start\",\n",
    "            x_end=\"cpu_end\",\n",
    "            y=\"name\",\n",
    "            # color=\"name\",\n",
    "            title=\"Recorded Functions Timeline\",\n",
    "            #color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "        )\n",
    "        fig.layout.xaxis.type = \"linear\"\n",
    "        fig.update_traces(\n",
    "            base=df_record[\"cpu_start\"].tolist(),\n",
    "            x   =df_record[\"delta\"].tolist(),\n",
    "            selector=dict(type=\"bar\")   # only affect bar traces\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    def create_combined_plot(self, timeline_fig, memory_fig, record_fig, height=1200, row_heights=[0.3, 0.4, 0.3]):\n",
    "        \"\"\"\n",
    "        Combine three subplots:\n",
    "          1. The recorded functions timeline.\n",
    "          2. The main memory events timeline.\n",
    "          3. The memory usage plot (accumulated or net).\n",
    "        All subplots share the same x-axis.\n",
    "        \n",
    "        Args:\n",
    "            timeline_fig: The main memory events timeline figure.\n",
    "            memory_fig: The memory usage figure.\n",
    "            record_fig: The recorded functions timeline figure.\n",
    "            height (int): Total height of the combined figure.\n",
    "            row_heights (list): Relative heights for the three subplots.\n",
    "        \"\"\"\n",
    "        fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "                            row_heights=row_heights)\n",
    "        # Row 1: Record function timeline.\n",
    "        for trace in record_fig.data:\n",
    "            fig.add_trace(trace, row=1, col=1)\n",
    "        # Row 2: Main memory events timeline.\n",
    "        for trace in timeline_fig.data:\n",
    "            fig.add_trace(trace, row=2, col=1)\n",
    "        # Row 3: Memory usage plot.\n",
    "        for trace in memory_fig.data:\n",
    "            fig.add_trace(trace, row=3, col=1)\n",
    "        fig.update_layout(coloraxis_colorbar=dict(title=\"Memory (GB)\"), height=height)\n",
    "        fig.update_xaxes(title_text=\"Time (ms)\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Memory Usage (GB)\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Event\", row=2, col=1, showticklabels=False)\n",
    "        return fig\n",
    "\n",
    "# =============================================\n",
    "# Example usage:\n",
    "# =============================================\n",
    "import torch\n",
    "import lightning as L \n",
    "from src.models.components.partmae_v6 import PARTMaskedAutoEncoderViT\n",
    "from src.data.components.transforms.multi_crop_v4 import ParametrizedMultiCropV4\n",
    "import timm\n",
    "from src.data.components.hf_dataset import HFDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Assume you have a CUDA model instance (an nn.Module) and an input tensor\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "fabric = L.Fabric(precision=\"bf16-mixed\", accelerator='gpu')\n",
    "# fabric = L.Fabric(accelerator='cpu')\n",
    "gV, lV = 2, 10\n",
    "V = gV + lV\n",
    "with fabric.init_module():\n",
    "    model = PARTMaskedAutoEncoderViT(\n",
    "        sampler=\"stratified_jittered\",\n",
    "        alpha_ts=0.8,\n",
    "        mask_ratio=0.75,\n",
    "        pos_mask_ratio=0.75,\n",
    "        alpha_t=0.75,\n",
    "        max_scale_ratio=6.0,\n",
    "        canonical_img_size=512,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        embed_dim=768,\n",
    "        decoder_embed_dim = 512,\n",
    "        decoder_depth = 8,\n",
    "        decoder_num_heads = 16,\n",
    "\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-6)\n",
    "\n",
    "\n",
    "model = model.cuda()\n",
    "B = 64\n",
    "transform = ParametrizedMultiCropV4(n_global_crops=gV, n_local_crops=lV)\n",
    "dataset = HFDataset(\"frgfm/imagenette\", \"160px\", transform=transform)\n",
    "train_dataloader = DataLoader(dataset, batch_size=B, shuffle=False)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "model.train()\n",
    "dataloader = fabric.setup_dataloaders(train_dataloader)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Create the profiler instance\n",
    "profiler = MemoryProfiler(filter_keyword=\"partmae_v6\")\n",
    "\n",
    "\n",
    "_ = model(*batch)\n",
    "with profiler.profile():\n",
    "    for _ in range(2):\n",
    "        with torch.autograd.profiler.record_function(\"forward\"):\n",
    "            out = model(*batch)\n",
    "        # with torch.autograd.profiler.record_function(\"backward\"):\n",
    "        #     fabric.backward(out[\"loss\"])\n",
    "\n",
    "# Extract events into a DataFrame\n",
    "df_events = profiler.extract_events_df()\n",
    "\n",
    "# Generate plots\n",
    "timeline_fig = profiler.plot_timeline()\n",
    "memory_fig = profiler.plot_net_memory_usage()  # or use plot_cumulative_memory()\n",
    "record_fig = profiler.plot_record_functions_timeline(record_fn_names=[\"forward\", \"backward\"])\n",
    "\n",
    "# Combine all three plots into one figure.\n",
    "combined_fig = profiler.create_combined_plot(timeline_fig, memory_fig, record_fig, height=1200, row_heights=[0.1, 0.5, 0.4])\n",
    "combined_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._dump_snapshot(\"../../mem.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
