# @package _global_

# to execute this experiment run:
# python src/main_pretrain.py experiment=pretrain/imagenet1k/partmae_v2/vit_b_16/fabric_a100

defaults:
  - pretrain/imagenet1k/partmae_v2/vit_b_16/_base
  - override /trainer: ddp

trainer:
  min_epochs: 200
  max_epochs: 800
  check_val_every_n_epoch: 100  # Added to match a100.yaml
  accumulate_grad_batches: 2
  deterministic: false
  devices: 4
  num_nodes: 1
  sync_batchnorm: true
  gradient_clip_val: 5.0
  log_every_n_steps: 50

model:
  optimizer:
    lr: 2.4e-3  # Keeping higher learning rate as it's likely optimized for fabric
  scheduler:
    warmup_t: 10
  net:
    sampler: stratified_jittered
    criterion: l1

data:
  batch_size: 2048  # Updated to match a100.yaml
  num_workers: 16
  prefetch_factor: 4  # Added to match a100.yaml
  cache_dir: /scratch-shared/dcanez/HF_HOME

# Enable float32 precision for matmul operations (improves training stability)
float32_matmul_precision: "high"

callbacks:
  checkpoint:
    _target_: src.callbacks.fabric.checkpoint.CheckpointCallback
    dirpath: ${paths.output_dir}/checkpoints
    save_last: true
    every_n_epochs: 100
    verbose: true

  reconstruction_logger:
    _target_: src.callbacks.fabric.reconstruction_logger.ReconstructionLoggerCallback
    log_every_n_steps: 50
    num_samples: 2

  metric_logger:
    _target_: src.callbacks.fabric.metric_logger.PARTMAEv2MetricLoggerCallback

logger:
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    project: "PART"
    name: ${task_name}/${experiment}
    tags: ${tags}
    group: ${task_name}

# Set task name for output directory
task_name: "pretrain_fabric_partmae_v2"

# Set tags for experiment tracking
tags: ["pretrain", "fabric", "partmae_v2", "vit_b_16"]