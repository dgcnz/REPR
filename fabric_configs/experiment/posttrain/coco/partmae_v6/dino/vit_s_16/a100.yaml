# @package _global_
defaults:
  - posttrain/coco/partmae_v6/dino/vit_s_16/_base

compile: False

total_batch_size: 512
ckpt_path: artifacts/dino_deitsmall16_pretrain_full_checkpoint.pth

# blr: 5e-6
blr: 5e-7

trainer:
  max_epochs: 150 # ~20h on 1xA100
  accelerator: gpu
  num_nodes: 1
  devices: 1

model:
  lambda_pmatch: 0
  lambda_pcr: 0
  lambda_ccr: 0.05
  lambda_cinv: 0.05
  lambda_pose: 0.9
  decoder_embed_dim: 256
  decoder_from_proj: true
  decoder_depth: 2
  cr_eps: 0.05

callbacks:
  metric_logger:
    every_n_steps: 10
  iter_timer:
    every_n_steps: 10
  checkpoint:
    save_perm_every_n_steps: ${eval:25 * ${steps_per_epoch}}
    save_temp_every_n_steps: ${steps_per_epoch}

scheduler:
  warmup_t: ${eval:10 * ${steps_per_epoch}} # 10 epochs
  t_initial: ${eval:${trainer.max_epochs} * ${steps_per_epoch}} # 150 epochs

data:
  root: /scratch-nvme/ml-datasets/coco/train/data

train_dataloader:
  batch_size: 512
  num_workers: 16

# evals:
#   - name: hummingbird
#     fn:
#       _target_: src.experiments.engine_hummingbird.eval
#       cfg:
#         model: partmae_v6
#         data: voc-mini
#         model.pretrained_cfg_overlay.state_dict.state_dict.f: ${paths.output_dir}/last.ckpt
