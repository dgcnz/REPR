{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, Callable\n",
    "\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.v2 as TTv2\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import logging\n",
    "import timm.data\n",
    "from datasets import Dataset\n",
    "from timm.data import Mixup, FastCollateMixup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "IMAGENET_TRANSFORMS = TTv2.Compose(\n",
    "    [\n",
    "        TTv2.ToImage(),\n",
    "        TTv2.ToDtype(torch.float32, scale=True),\n",
    "        TTv2.RGB(),\n",
    "        TTv2.Resize(size=(224, 224), interpolation=TTv2.InterpolationMode.BICUBIC),\n",
    "        TTv2.CenterCrop(size=(224, 224)),\n",
    "        TTv2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "DEFAULT_TEST_TRANSFORM = TTv2.Compose(\n",
    "    [\n",
    "        TTv2.ToImage(),\n",
    "        TTv2.RGB(),\n",
    "        TTv2.ToDtype(torch.float32, scale=True),\n",
    "        TTv2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "CIFAR10_TRAIN_TRANSFORM = timm.data.create_transform(\n",
    "    input_size=32,\n",
    "    is_training=True,\n",
    "    color_jitter=0.4,\n",
    "    auto_augment=\"rand-m9-mstd0.5-inc1\",\n",
    "    interpolation=\"bicubic\",\n",
    "    re_prob=0,  # 0.25 when finetuning for classification\n",
    "    re_mode=\"pixel\",\n",
    "    re_count=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def to_hf_transform(\n",
    "    transform: Callable, img_key: str = \"image\", label_key: str | None = \"label\"\n",
    ") -> Callable:\n",
    "    if transform is None:\n",
    "        transform = TTv2.ToTensor()\n",
    "\n",
    "    def _transform(batch):\n",
    "        out = dict()\n",
    "        if label_key is not None:\n",
    "            out[label_key] = batch[label_key]\n",
    "        return out | {img_key: [transform(x.convert('RGB')) for x in batch[img_key]]}\n",
    "\n",
    "    return _transform\n",
    "\n",
    "\n",
    "class HFDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str = \"uoft-cs/cifar10\",\n",
    "        train_transform: Callable = DEFAULT_TEST_TRANSFORM,\n",
    "        test_transform: Callable = DEFAULT_TEST_TRANSFORM,\n",
    "        img_key: str = \"image\",\n",
    "        label_key: str | None = None,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        val_fraction: float = None,  # split train into train/val\n",
    "        test_fraction: float = None,  # split val into val/test\n",
    "        mixup: Mixup | None = None,\n",
    "        cache_dir: str | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a `HFDataModule`.\n",
    "\n",
    "        :param batch_size: The batch size. Defaults to `64`.\n",
    "        :param num_workers: The number of workers. Defaults to `0`.\n",
    "        :param pin_memory: Whether to pin memory. Defaults to `False`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # also ensures init params will be stored in ckpt\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_key = img_key\n",
    "        self.label_key = label_key\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "        self.train_transform = to_hf_transform(train_transform, img_key, label_key)\n",
    "        self.test_transform = to_hf_transform(test_transform, img_key, label_key)\n",
    "\n",
    "        self.batch_size_per_device = batch_size\n",
    "        self.cli_logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Download data if needed. Lightning ensures that `self.prepare_data()` is called only\n",
    "        within a single process on CPU, so you can safely add your downloading logic within. In\n",
    "        case of multi-node training, the execution of this hook depends upon\n",
    "        `self.prepare_data_per_node()`.\n",
    "\n",
    "        Do not use it to assign state (self.x = y).\n",
    "        \"\"\"\n",
    "        logging.info(f\"Preparing {self.hparams.dataset_name} dataset.\")\n",
    "        if self.hparams.dataset_name == self.dataset_name:\n",
    "            logging.warning(\n",
    "                \"Before running this, make sure you use the HF CLI to download the data:\\n\"\n",
    "                \"huggingface-cli download ILSVRC/imagenet-1k --repo-type dataset\"\n",
    "            )\n",
    "\n",
    "        _ = load_dataset(self.hparams.dataset_name, cache_dir=self.hparams.cache_dir)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n",
    "\n",
    "        This method is called by Lightning before `trainer.fit()`, `trainer.validate()`, `trainer.test()`, and\n",
    "        `trainer.predict()`, so be careful not to execute things like random split twice! Also, it is called after\n",
    "        `self.prepare_data()` and there is a barrier in between which ensures that all the processes proceed to\n",
    "        `self.setup()` once the data is prepared and available for use.\n",
    "\n",
    "        :param stage: The stage to setup. Either `\"fit\"`, `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to ``None``.\n",
    "        \"\"\"\n",
    "        # Divide batch size by the number of devices.\n",
    "        if self.trainer is not None:\n",
    "            if self.hparams.batch_size % self.trainer.world_size != 0:\n",
    "                raise RuntimeError(\n",
    "                    f\"Batch size ({self.hparams.batch_size}) is not divisible by the number of devices ({self.trainer.world_size}).\"\n",
    "                )\n",
    "            self.batch_size_per_device = (\n",
    "                self.hparams.batch_size // self.trainer.world_size\n",
    "            )\n",
    "\n",
    "        # load and split datasets only if not loaded already\n",
    "        if not self.data_train and not self.data_val and not self.data_test:\n",
    "            ds = load_dataset(\n",
    "                self.dataset_name, cache_dir=self.hparams.cache_dir\n",
    "            ).with_format(\"torch\")\n",
    "\n",
    "            if \"validation\" in ds:\n",
    "                self.data_train = ds[\"train\"]\n",
    "                self.data_val = ds[\"validation\"]\n",
    "            elif \"val\" in ds:\n",
    "                self.data_train = ds[\"train\"]\n",
    "                self.data_val = ds[\"val\"]\n",
    "            else:\n",
    "                # split train into train/val\n",
    "                if self.hparams.val_fraction is None:\n",
    "                    raise ValueError(\n",
    "                        \"Validation fraction must be provided if no validation set is found.\"\n",
    "                    )\n",
    "                splits = ds[\"train\"].train_test_split(\n",
    "                    test_size=self.hparams.val_fraction\n",
    "                )\n",
    "                self.data_train = splits[\"train\"]\n",
    "                self.data_val = splits[\"test\"]\n",
    "\n",
    "            # split validation set into validation and test sets\n",
    "            if self.hparams.test_fraction is None:\n",
    "                self.data_test = ds[\"test\"]\n",
    "            else:\n",
    "                splits = self.data_val.train_test_split(\n",
    "                    test_size=self.hparams.test_fraction\n",
    "                )\n",
    "                self.data_val = splits[\"train\"]\n",
    "                self.data_test = splits[\"test\"]\n",
    "\n",
    "            self.data_train.set_transform(self.train_transform)\n",
    "            self.data_val.set_transform(self.train_transform)\n",
    "            self.data_test.set_transform(self.test_transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the train dataloader.\n",
    "\n",
    "        :return: The train dataloader.\n",
    "        \"\"\"\n",
    "        collate_fn = None\n",
    "        if self.hparams.mixup is not None:\n",
    "            collate_fn = FastCollateMixup(self.hparams.mixup)\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the validation dataloader.\n",
    "\n",
    "        :return: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the test dataloader.\n",
    "\n",
    "        :return: The test dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ILSVRC/imagenet-1k\")\n",
    "ds.rename_column()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
