{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "import json, io, base64\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, Javascript\n",
    "from src.utils.analysis.clmim_hook import ActivationCache\n",
    "import ipywidgets as widgets\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partmae():\n",
    "    return timm.create_model(\n",
    "        \"vit_base_patch16_224\",\n",
    "        pretrained=True,\n",
    "        pretrained_cfg_overlay={\n",
    "            \"file\": \"../../artifacts/model-2knf0d16:v0/backbone.ckpt\"\n",
    "        },\n",
    "        pretrained_strict=False,\n",
    "    ).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vit():\n",
    "    return timm.create_model(\n",
    "        \"vit_base_patch16_224\",\n",
    "        pretrained=True\n",
    "    ).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mae():\n",
    "    return timm.create_model('vit_base_patch16_224.mae', pretrained=True).cuda().eval()\n",
    "\n",
    "def load_dino():\n",
    "    return timm.create_model('vit_base_patch16_224.dino', pretrained=True).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dora():\n",
    "    return (\n",
    "        torch.hub.load(\n",
    "            \"dgcnz/DoRA_ICLR24\", model=\"vit_small_patch16_224_dora_wt_venice_ep100\"\n",
    "        )\n",
    "        .cuda()\n",
    "        .eval()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = load_partmae()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = timm.create_model('vit_base_patch16_224', pretrained=True).eval().cuda()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrgfm/imagenette\u001b[39m\u001b[38;5;124m\"\u001b[39m , split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m160px\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataset[:\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# model = load_partmae()\n",
    "# model = timm.create_model('vit_base_patch16_224', pretrained=True).eval().cuda()\n",
    "\n",
    "dataset = load_dataset(\"frgfm/imagenette\" , split=\"validation\", name=\"160px\")\n",
    "batch = dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interactive_html(all_attn_maps, image_base64, num_layers, num_heads):\n",
    "    all_attns_json = json.dumps(all_attn_maps)\n",
    "    html_code = f'''\n",
    "    <h3>Hover over the top image to update attention maps</h3>\n",
    "    <canvas id=\"input-image\" width=\"224\" height=\"224\" style=\"border:1px solid #000;\"></canvas>\n",
    "    <div id=\"attention-grid\" style=\"display: grid; grid-template-columns: repeat({num_heads}, auto); grid-gap: 5px; margin-top: 10px;\"></div>\n",
    "    \n",
    "    <script>\n",
    "    var allAttentionMaps = {all_attns_json}; \n",
    "    var numLayers = {num_layers};\n",
    "    var numHeads = {num_heads};\n",
    "    var canvasSize = 224;\n",
    "    var gridSize = 14;\n",
    "    var patchSize = canvasSize / gridSize;\n",
    "    \n",
    "    // Define viridis colormap using key points.\n",
    "    function getViridisColor(t) {{\n",
    "        t = Math.min(Math.max(t, 0), 1);\n",
    "        var viridis = [\n",
    "            {{t: 0.0, color: [68, 1, 84]}},\n",
    "            {{t: 0.125, color: [71, 44, 122]}},\n",
    "            {{t: 0.25, color: [59, 81, 139]}},\n",
    "            {{t: 0.375, color: [44, 113, 142]}},\n",
    "            {{t: 0.5, color: [33, 144, 141]}},\n",
    "            {{t: 0.625, color: [39, 173, 129]}},\n",
    "            {{t: 0.75, color: [92, 200, 99]}},\n",
    "            {{t: 0.875, color: [170, 220, 50]}},\n",
    "            {{t: 1.0, color: [253, 231, 37]}}\n",
    "        ];\n",
    "        for (var i = 0; i < viridis.length - 1; i++) {{\n",
    "            if (t >= viridis[i].t && t <= viridis[i+1].t) {{\n",
    "                var ratio = (t - viridis[i].t) / (viridis[i+1].t - viridis[i].t);\n",
    "                var r = Math.floor(viridis[i].color[0] + ratio * (viridis[i+1].color[0] - viridis[i].color[0]));\n",
    "                var g = Math.floor(viridis[i].color[1] + ratio * (viridis[i+1].color[1] - viridis[i].color[1]));\n",
    "                var b = Math.floor(viridis[i].color[2] + ratio * (viridis[i+1].color[2] - viridis[i].color[2]));\n",
    "                return [r, g, b];\n",
    "            }}\n",
    "        }}\n",
    "        return viridis[viridis.length - 1].color;\n",
    "    }}\n",
    "    \n",
    "    function drawTopImage() {{\n",
    "        var canvas = document.getElementById(\"input-image\");\n",
    "        var ctx = canvas.getContext(\"2d\");\n",
    "        var img = new Image();\n",
    "        img.onload = function() {{\n",
    "            ctx.drawImage(img, 0, 0, canvasSize, canvasSize);\n",
    "        }};\n",
    "        img.src = \"data:image/jpeg;base64,{image_base64}\";\n",
    "    }}\n",
    "    \n",
    "    function createGrid() {{\n",
    "        var gridDiv = document.getElementById(\"attention-grid\");\n",
    "        gridDiv.innerHTML = \"\";\n",
    "        for(var l=0; l<numLayers; l++) {{\n",
    "            for(var h=0; h<numHeads; h++) {{\n",
    "                var canvas = document.createElement(\"canvas\");\n",
    "                canvas.id = \"attmap-\" + l + \"-\" + h;\n",
    "                canvas.width = gridSize;\n",
    "                canvas.height = gridSize;\n",
    "                canvas.style.width = (canvasSize/4) + \"px\";\n",
    "                canvas.style.height = (canvasSize/4) + \"px\";\n",
    "                canvas.style.border = \"1px solid #000\";\n",
    "                gridDiv.appendChild(canvas);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    function reshapeToMatrix(arr, size) {{\n",
    "        var matrix = [];\n",
    "        for(var i=0; i<size; i++) {{\n",
    "            matrix.push(arr.slice(i*size, (i+1)*size));\n",
    "        }}\n",
    "        return matrix;\n",
    "    }}\n",
    "    \n",
    "    function drawAttentionHeatmap(canvasId, dataMatrix) {{\n",
    "        var canvas = document.getElementById(canvasId);\n",
    "        var ctx = canvas.getContext(\"2d\");\n",
    "        var size = dataMatrix.length;\n",
    "        var flat = dataMatrix.flat();\n",
    "        var minVal = Math.min(...flat);\n",
    "        var maxVal = Math.max(...flat);\n",
    "        var imgData = ctx.createImageData(size, size);\n",
    "        \n",
    "        for(var i=0; i<size; i++) {{\n",
    "            for(var j=0; j<size; j++) {{\n",
    "                var value = dataMatrix[i][j];\n",
    "                var normVal = (maxVal - minVal) ? (value - minVal) / (maxVal - minVal) : 0;\n",
    "                var rgb = getViridisColor(normVal);\n",
    "                var index = (i * size + j) * 4;\n",
    "                imgData.data[index] = rgb[0];\n",
    "                imgData.data[index+1] = rgb[1];\n",
    "                imgData.data[index+2] = rgb[2];\n",
    "                imgData.data[index+3] = 255;\n",
    "            }}\n",
    "        }}\n",
    "        ctx.putImageData(imgData, 0, 0);\n",
    "    }}\n",
    "    \n",
    "    function updateAttentionMaps(queryPatch) {{\n",
    "        var tokenIndex = queryPatch + 1;\n",
    "        for(var l=0; l<numLayers; l++) {{\n",
    "            for(var h=0; h<numHeads; h++) {{\n",
    "                var attnVector = allAttentionMaps[l][h][tokenIndex].slice(1);\n",
    "                var heatmap = reshapeToMatrix(attnVector, gridSize);\n",
    "                drawAttentionHeatmap(\"attmap-\" + l + \"-\" + h, heatmap);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    document.getElementById(\"input-image\").addEventListener(\"mousemove\", function(event) {{\n",
    "        var rect = this.getBoundingClientRect();\n",
    "        var x = Math.floor((event.clientX - rect.left) / patchSize);\n",
    "        var y = Math.floor((event.clientY - rect.top) / patchSize);\n",
    "        var patchIndex = y * gridSize + x;\n",
    "        updateAttentionMaps(patchIndex);\n",
    "    }});\n",
    "    \n",
    "    drawTopImage();\n",
    "    createGrid();\n",
    "    updateAttentionMaps(0);\n",
    "    </script>\n",
    "    '''\n",
    "    display(HTML(html_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get attention maps via a hook; each returned tensor has shape (1, heads, tokens, tokens)\n",
    "def get_attention_maps(model, img_tensor):\n",
    "    cache = ActivationCache()\n",
    "    cache.hook(model)\n",
    "    with torch.no_grad():\n",
    "        _ = model(img_tensor)\n",
    "    attn = cache.get_attns()\n",
    "    attn_list = []\n",
    "    for layer_attn in attn:\n",
    "        # Remove batch dimension and convert tensor to nested lists for JSON serialization\n",
    "        attn_layer = layer_attn.squeeze(0).cpu().tolist()\n",
    "        attn_list.append(attn_layer)\n",
    "    return attn_list\n",
    "\n",
    "# Preprocess a PIL image: returns tensor, bytes, and the original PIL image.\n",
    "def preprocess_image(pil_image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    img_tensor = transform(pil_image).unsqueeze(0)\n",
    "    with io.BytesIO() as buf:\n",
    "        pil_image.save(buf, format=\"JPEG\")\n",
    "        image_bytes = buf.getvalue()\n",
    "    return img_tensor, image_bytes, pil_image\n",
    "\n",
    "# Generate interactive HTML.\n",
    "\n",
    "# Instead of a file uploader, use a dropdown to select one of the batch images.\n",
    "dropdown = widgets.Dropdown(options=[(f\"Image {i}\", i) for i in range(len(batch))],\n",
    "                            description=\"Select image:\")\n",
    "\n",
    "models = {\n",
    "    \"vit\": load_vit,\n",
    "    \"partmae\": load_partmae,\n",
    "    \"dora\": load_dora,\n",
    "    \"dino\": load_dino,\n",
    "    \"mae\": load_mae,\n",
    "}\n",
    "model = models[\"vit\"]()\n",
    "image_id = 0\n",
    "\n",
    "model_dropdown = widgets.Dropdown(options=[(name, name) for name in models.keys()],\n",
    "                                  description=\"Select model:\")\n",
    "\n",
    "def process_image(index):\n",
    "    # Assuming the key for images in the dataset is \"image\"\n",
    "    pil_image = batch[\"image\"][index]\n",
    "    logging.info(\"[Image Preprocessing] START\")\n",
    "    t0 = time.time()\n",
    "    img_tensor, image_bytes, _ = preprocess_image(pil_image)\n",
    "    t1 = time.time()\n",
    "    logging.info(f\"[Image Preprocessing] END (time: {t1 - t0:.2f}s)\")\n",
    "    logging.info(f\"[Attention Maps] START\")\n",
    "    attn_maps = get_attention_maps(model, img_tensor.cuda())\n",
    "    t2 = time.time()\n",
    "    logging.info(f\"[Attention Maps] END (time: {t2 - t1:.2f}s)\")\n",
    "    logging.info(\"[Interactive HTML] START\")\n",
    "    image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    num_layers = len(attn_maps)\n",
    "    num_heads = len(attn_maps[0]) if num_layers > 0 else 0\n",
    "    generate_interactive_html(attn_maps, image_base64, num_layers, num_heads)\n",
    "    t3 = time.time()\n",
    "    logging.info(f\"[Interactive HTML] END (time: {t3 - t2:.2f}s)\")\n",
    "\n",
    "def dropdown_changed(change):\n",
    "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "        global image_id\n",
    "        image_id = change[\"new\"]\n",
    "        logging.info(f\"Processing image {image_id}\")\n",
    "        process_image(image_id)\n",
    "        logging.info(f\"Image {image_id} processed\")\n",
    "\n",
    "def model_dropdown_changed(change):\n",
    "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "        global model\n",
    "        logging.info(f\"[Model Loading] START\")\n",
    "        t0 = time.time()\n",
    "        logging.info(f\"Loading model {change['new']}\")\n",
    "        model = models[change[\"new\"]]()\n",
    "        t1 = time.time()\n",
    "        logging.info(f\"[Model Loading] END (time: {t1 - t0:.2f}s)\")\n",
    "        logging.info(f\"[Image Processing] START\")\n",
    "        t2 = time.time()\n",
    "        process_image(image_id)\n",
    "        t3 = time.time()\n",
    "        logging.info(f\"[Image Processing] END (time: {t3 - t2:.2f}s)\")\n",
    "\n",
    "display(dropdown)\n",
    "display(model_dropdown)\n",
    "\n",
    "dropdown.observe(dropdown_changed, names=\"value\")\n",
    "model_dropdown.observe(model_dropdown_changed, names=\"value\")\n",
    "# Process the initially selected image.\n",
    "process_image(dropdown.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
