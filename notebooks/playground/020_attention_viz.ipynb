{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0.dev20250122+cu126 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "import json, io, base64\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, Javascript\n",
    "from src.utils.analysis.clmim_hook import ActivationCache\n",
    "import ipywidgets as widgets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partmae():\n",
    "    return timm.create_model(\n",
    "        \"vit_base_patch16_224\",\n",
    "        pretrained=True,\n",
    "        pretrained_cfg_overlay={\n",
    "            \"file\": \"../../artifacts/model-2knf0d16:v0/backbone.ckpt\"\n",
    "        },\n",
    "        pretrained_strict=False,\n",
    "    ).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from file (../../artifacts/model-2knf0d16:v0/backbone.ckpt)\n",
      "INFO:timm.models._helpers:Loaded  from checkpoint '../../artifacts/model-2knf0d16:v0/backbone.ckpt'\n",
      "INFO:timm.models._builder:Missing keys (head.weight, head.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "# model = timm.create_model('vit_base_patch16_224', pretrained=True).eval().cuda()\n",
    "model = load_partmae()\n",
    "dataset = load_dataset(\"frgfm/imagenette\" , split=\"validation\", name=\"160px\")\n",
    "batch = dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interactive_html(all_attn_maps, image_base64, num_layers, num_heads):\n",
    "    all_attns_json = json.dumps(all_attn_maps)\n",
    "    html_code = f'''\n",
    "    <h3>Hover over the top image to update attention maps</h3>\n",
    "    <canvas id=\"input-image\" width=\"224\" height=\"224\" style=\"border:1px solid #000;\"></canvas>\n",
    "    <div id=\"attention-grid\" style=\"display: grid; grid-template-columns: repeat({num_heads}, auto); grid-gap: 5px; margin-top: 10px;\"></div>\n",
    "    \n",
    "    <script>\n",
    "    var allAttentionMaps = {all_attns_json}; \n",
    "    var numLayers = {num_layers};\n",
    "    var numHeads = {num_heads};\n",
    "    var canvasSize = 224;\n",
    "    var gridSize = 14;\n",
    "    var patchSize = canvasSize / gridSize;\n",
    "    \n",
    "    // Define viridis colormap using key points.\n",
    "    function getViridisColor(t) {{\n",
    "        t = Math.min(Math.max(t, 0), 1);\n",
    "        var viridis = [\n",
    "            {{t: 0.0, color: [68, 1, 84]}},\n",
    "            {{t: 0.125, color: [71, 44, 122]}},\n",
    "            {{t: 0.25, color: [59, 81, 139]}},\n",
    "            {{t: 0.375, color: [44, 113, 142]}},\n",
    "            {{t: 0.5, color: [33, 144, 141]}},\n",
    "            {{t: 0.625, color: [39, 173, 129]}},\n",
    "            {{t: 0.75, color: [92, 200, 99]}},\n",
    "            {{t: 0.875, color: [170, 220, 50]}},\n",
    "            {{t: 1.0, color: [253, 231, 37]}}\n",
    "        ];\n",
    "        for (var i = 0; i < viridis.length - 1; i++) {{\n",
    "            if (t >= viridis[i].t && t <= viridis[i+1].t) {{\n",
    "                var ratio = (t - viridis[i].t) / (viridis[i+1].t - viridis[i].t);\n",
    "                var r = Math.floor(viridis[i].color[0] + ratio * (viridis[i+1].color[0] - viridis[i].color[0]));\n",
    "                var g = Math.floor(viridis[i].color[1] + ratio * (viridis[i+1].color[1] - viridis[i].color[1]));\n",
    "                var b = Math.floor(viridis[i].color[2] + ratio * (viridis[i+1].color[2] - viridis[i].color[2]));\n",
    "                return [r, g, b];\n",
    "            }}\n",
    "        }}\n",
    "        return viridis[viridis.length - 1].color;\n",
    "    }}\n",
    "    \n",
    "    function drawTopImage() {{\n",
    "        var canvas = document.getElementById(\"input-image\");\n",
    "        var ctx = canvas.getContext(\"2d\");\n",
    "        var img = new Image();\n",
    "        img.onload = function() {{\n",
    "            ctx.drawImage(img, 0, 0, canvasSize, canvasSize);\n",
    "        }};\n",
    "        img.src = \"data:image/jpeg;base64,{image_base64}\";\n",
    "    }}\n",
    "    \n",
    "    function createGrid() {{\n",
    "        var gridDiv = document.getElementById(\"attention-grid\");\n",
    "        gridDiv.innerHTML = \"\";\n",
    "        for(var l=0; l<numLayers; l++) {{\n",
    "            for(var h=0; h<numHeads; h++) {{\n",
    "                var canvas = document.createElement(\"canvas\");\n",
    "                canvas.id = \"attmap-\" + l + \"-\" + h;\n",
    "                canvas.width = gridSize;\n",
    "                canvas.height = gridSize;\n",
    "                canvas.style.width = (canvasSize/4) + \"px\";\n",
    "                canvas.style.height = (canvasSize/4) + \"px\";\n",
    "                canvas.style.border = \"1px solid #000\";\n",
    "                gridDiv.appendChild(canvas);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    function reshapeToMatrix(arr, size) {{\n",
    "        var matrix = [];\n",
    "        for(var i=0; i<size; i++) {{\n",
    "            matrix.push(arr.slice(i*size, (i+1)*size));\n",
    "        }}\n",
    "        return matrix;\n",
    "    }}\n",
    "    \n",
    "    function drawAttentionHeatmap(canvasId, dataMatrix) {{\n",
    "        var canvas = document.getElementById(canvasId);\n",
    "        var ctx = canvas.getContext(\"2d\");\n",
    "        var size = dataMatrix.length;\n",
    "        var flat = dataMatrix.flat();\n",
    "        var minVal = Math.min(...flat);\n",
    "        var maxVal = Math.max(...flat);\n",
    "        var imgData = ctx.createImageData(size, size);\n",
    "        \n",
    "        for(var i=0; i<size; i++) {{\n",
    "            for(var j=0; j<size; j++) {{\n",
    "                var value = dataMatrix[i][j];\n",
    "                var normVal = (maxVal - minVal) ? (value - minVal) / (maxVal - minVal) : 0;\n",
    "                var rgb = getViridisColor(normVal);\n",
    "                var index = (i * size + j) * 4;\n",
    "                imgData.data[index] = rgb[0];\n",
    "                imgData.data[index+1] = rgb[1];\n",
    "                imgData.data[index+2] = rgb[2];\n",
    "                imgData.data[index+3] = 255;\n",
    "            }}\n",
    "        }}\n",
    "        ctx.putImageData(imgData, 0, 0);\n",
    "    }}\n",
    "    \n",
    "    function updateAttentionMaps(queryPatch) {{\n",
    "        var tokenIndex = queryPatch + 1;\n",
    "        for(var l=0; l<numLayers; l++) {{\n",
    "            for(var h=0; h<numHeads; h++) {{\n",
    "                var attnVector = allAttentionMaps[l][h][tokenIndex].slice(1);\n",
    "                var heatmap = reshapeToMatrix(attnVector, gridSize);\n",
    "                drawAttentionHeatmap(\"attmap-\" + l + \"-\" + h, heatmap);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    document.getElementById(\"input-image\").addEventListener(\"mousemove\", function(event) {{\n",
    "        var rect = this.getBoundingClientRect();\n",
    "        var x = Math.floor((event.clientX - rect.left) / patchSize);\n",
    "        var y = Math.floor((event.clientY - rect.top) / patchSize);\n",
    "        var patchIndex = y * gridSize + x;\n",
    "        updateAttentionMaps(patchIndex);\n",
    "    }});\n",
    "    \n",
    "    drawTopImage();\n",
    "    createGrid();\n",
    "    updateAttentionMaps(0);\n",
    "    </script>\n",
    "    '''\n",
    "    display(HTML(html_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (The \"model\", \"model_name\", and \"batch\" variables are already defined in previous cells)\n",
    "\n",
    "# Function to get attention maps via a hook; each returned tensor has shape (1, heads, tokens, tokens)\n",
    "def get_attention_maps(model, img_tensor):\n",
    "    cache = ActivationCache()\n",
    "    cache.hook(model)\n",
    "    with torch.no_grad():\n",
    "        _ = model(img_tensor)\n",
    "    attn = cache.get_attns()\n",
    "    attn_list = []\n",
    "    for layer_attn in attn:\n",
    "        # Remove batch dimension and convert tensor to nested lists for JSON serialization\n",
    "        attn_layer = layer_attn.squeeze(0).cpu().tolist()\n",
    "        attn_list.append(attn_layer)\n",
    "    return attn_list\n",
    "\n",
    "# Preprocess a PIL image: returns tensor, bytes, and the original PIL image.\n",
    "def preprocess_image(pil_image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    img_tensor = transform(pil_image).unsqueeze(0)\n",
    "    with io.BytesIO() as buf:\n",
    "        pil_image.save(buf, format=\"JPEG\")\n",
    "        image_bytes = buf.getvalue()\n",
    "    return img_tensor, image_bytes, pil_image\n",
    "\n",
    "# Generate interactive HTML.\n",
    "\n",
    "# Instead of a file uploader, use a dropdown to select one of the batch images.\n",
    "dropdown = widgets.Dropdown(options=[(f\"Image {i}\", i) for i in range(len(batch))],\n",
    "                            description=\"Select image:\")\n",
    "display(dropdown)\n",
    "\n",
    "def process_image(index):\n",
    "    # Assuming the key for images in the dataset is \"image\"\n",
    "    pil_image = batch[\"image\"][index]\n",
    "    img_tensor, image_bytes, _ = preprocess_image(pil_image)\n",
    "    attn_maps = get_attention_maps(model, img_tensor.cuda())\n",
    "    image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    num_layers = len(attn_maps)\n",
    "    num_heads = len(attn_maps[0]) if num_layers > 0 else 0\n",
    "    generate_interactive_html(attn_maps, image_base64, num_layers, num_heads)\n",
    "\n",
    "def dropdown_changed(change):\n",
    "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "        process_image(change[\"new\"])\n",
    "\n",
    "dropdown.observe(dropdown_changed, names=\"value\")\n",
    "# Process the initially selected image.\n",
    "process_image(dropdown.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
